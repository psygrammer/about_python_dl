{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. What is deep learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 어바웃 파이썬 : 딥러닝 with Keras [1, 2]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 차례\n",
    "* 1.1 Artificial intelligence, machine learning, and deep learning\n",
    "* 1.2 Before deep learning: a brief history of machine learning\n",
    "* 1.3 Why deep learning? Why now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Artificial intelligence, machine learning, and deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap01.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Artificial intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the effort to automate intellectual tasks normally per- formed by humans.\n",
    "* symbolic AI\n",
    "    - expert systems\n",
    "* machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap02.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Learning representations from data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "\n",
    "> * [4] Santford's class(2017) / CS231n: Convolutional Neural Networks for Visual Recognition / Lecture 3: Loss Functions and Optimization - http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture3.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* deep learning\n",
    "    - To define deep learning and understand the difference between deep learning and other machine-learning approaches, first we need some idea of what machine-learning algorithms do.\n",
    "* machine learning\n",
    "    - I just stated that machine learning discovers rules to execute a data-processing task, given examples of what’s expected. So, to do machine learning, we need three things:\n",
    "        - Input data points\n",
    "        - Examples of the expected output\n",
    "        - A way to measure whether the algorithm is doing a good job\n",
    "* representations\n",
    "    - A machine-learning model transforms its input data into meaningful outputs, a process that is “learned” from exposure to known examples of inputs and outputs.\n",
    "        - meaningfully transform data \n",
    "            - representations\n",
    "* Learning\n",
    "    - Learning, in the context of machine learning, describes an automatic search process for better representations.\n",
    "* hypothesis space\n",
    "    - All machine-learning algorithms consist of automatically finding such transformations that turn data into more-useful representations for a given task.\n",
    "        - coordinate changes\n",
    "        - linear projections\n",
    "        - translations\n",
    "        - nonlinear operations \n",
    "    - Machine-learning algorithms aren’t usually creative in finding these transformations\n",
    "    - they’re merely searching through a predefined set of operations, called a hypothesis space.\n",
    "* <font color=\"red\">So that’s what machine learning is, technically: searching for useful representations of some input data, within a predefined space of possibilities, using guidance from a feedback signal.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap03.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap04.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 The \"deep\" in deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* deep\n",
    "    - layered representations learning = hierarchical representations learning\n",
    "* neural network\n",
    "    - In deep learning, these layered representations are (almost always) learned via models called neural networks, structured in literal layers stacked on top of each other.\n",
    "* brain\n",
    "    - The term neural network is a reference to neurobiology, but although some of the central concepts in deep learning were developed in part by drawing inspiration from our understanding of the brain, deep-learning models are not models of the brain.\n",
    "    - For our purposes, deep learning is a mathematical framework for learning representations from data.\n",
    "* <font color=\"red\">So that’s what deep learning is, technically: a multistage way to learn data representations. It’s a simple idea—but, as it turns out, very simple mechanisms, sufficiently scaled, can end up looking like magic.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap05.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap06.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5 Understanding how deep learning works, in three figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "\n",
    ">* [3] Santford's class(2017) / CS231n: Convolutional Neural Networks for Visual Recognition / Lecture2 Image Classsification Pipeline - http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture2.pdf\n",
    "* [4] Santford's class(2017) / CS231n: Convolutional Neural Networks for Visual Recognition / Lecture 3: Loss Functions and Optimization - http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture3.pdf\n",
    "* [5] Santford's class(2017) / CS231n: Convolutional Neural Networks for Visual Recognition / Lecture 4: Backpropagation and Neural Networks - http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture4.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* input-to-target mapping\n",
    "    - via a deep sequence of simple data transformations (layers)\n",
    "* data transformations (layers)\n",
    "    - these data transformations are learned by exposure to examples.\n",
    "* weights\n",
    "    - = parameters \n",
    "    - The specification of what a layer does to its input data is stored in the layer’s weights, which in essence are a bunch of numbers.\n",
    "    - parameterized\n",
    "        - In technical terms, we’d say that the transformation implemented by a layer is parameterized by its weights\n",
    "* learning\n",
    "    - In this context, learning means finding a set of values for the weights of all layers in a network, such that the network will correctly map example inputs to their associated targets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap07.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* loss function\n",
    "    - = objective function\n",
    "    - The loss function takes \n",
    "        - the <font color=\"blue\">predictions</font> of the network and \n",
    "        - the <font color=\"blue\">true target</font> (what you wanted the network to output) and \n",
    "        - computes a <font color=\"blue\">distance score</font>, \n",
    "            - capturing how well the network has done on this specific example (see figure 1.8).\n",
    "    - To control the output of a neural network\n",
    "        - a deep neural network can contain tens of millions of parameters. \n",
    "        - Finding the correct value for all of them may seem like a daunting task, especially given that modifying the value of one parameter will affect the behavior of all the others!\n",
    "        - To control something, first you need to be able to observe it. \n",
    "        - To control the output of a neural network, you need to be able to measure how far this output is from what you expected.\n",
    "        - This is the job of the loss function of the network, also called the objective function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap08.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* adjustment\n",
    "    - use this score as a feedback signal \n",
    "        - to adjust the value of the weights a little, \n",
    "        - in a direction that will lower the loss score \n",
    "            - for the current example (see figure 1.9).\n",
    "* Backpropagation\n",
    "    - the central algorithm in deep learning.\n",
    "    - optimizer\n",
    "        - This adjustment is the job of the optimizer, \n",
    "            - which implements what’s called the Backpropagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap09.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.6 What deep learning has achieved so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Near-human-level image classification\n",
    "* Near-human-level speech recognition\n",
    "* Near-human-level handwriting transcription \n",
    "* Improved machine translation\n",
    "* Improved text-to-speech conversion\n",
    "* Digital assistants such as Google Now and Amazon Alexa\n",
    "* Near-human-level autonomous driving\n",
    "* Improved ad targeting, as used by Google, Baidu, and Bing  Improved search results on the web\n",
    "* Ability to answer natural-language questions\n",
    "* Superhuman Go playing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.7 Don’t believe the short-term hype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don’t believe the short-term hype, but do believe in the long-term vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.8 The promise of AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Before deep learning: a brief history of machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Probabilistic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Naive Bayes\n",
    "* logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Early neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Kernel methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Kernel methods\n",
    "    - support vector machine (SVM)\n",
    "        - decision boundaries \n",
    "    - kernel trick\n",
    "        - kernel function\n",
    "* feature engineering\n",
    "    - But SVMs proved hard to scale to large datasets and didn’t provide good results for perceptual problems such as image classification. Because an SVM is a shallow method, applying an SVM to perceptual problems requires first extracting useful representations manually (a step called feature engineering), which is difficult and brittle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Decision trees, random forests, and gradient boosting machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap11.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 Back to neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.6 What makes deep learning different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* feature engineering \n",
    "    - Deep learning, on the other hand, completely automates this step\n",
    "        - end-to-end deep-learning model\n",
    "            - with deep learning, you learn all features in one pass rather than having to engineer them yourself. This has greatly simplified machine-learning workflows, often replac- ing sophisticated multistage pipelines with a single, simple, end-to-end deep-learning model.\n",
    "    - jointly\n",
    "        - shallow learning\n",
    "            - You may ask, if the crux of the issue is to have multiple successive layers of representations, could shallow methods be applied repeatedly to emulate the effects of deep learning?\n",
    "            - In practice, there are fast-diminishing returns to successive applications of shallow-learning methods, \n",
    "                - because the optimal first representation layer in a three-layer model isn’t the optimal first layer in a one-layer or two-layer model.\n",
    "        - deep learning\n",
    "            - What is transformative about deep learning is that it allows a model to learn all layers of representation jointly, at the same time, rather than in succession (greedily, as it’s called).\n",
    "* These are the two essential characteristics of how deep learning learns from data:\n",
    "    - <font color=\"blue\">incremental</font>\n",
    "        - the incremental, layer-by-layer way in which increasingly complex representations are developed,\n",
    "    - <font color=\"blue\">jointly</font>\n",
    "        - and the fact that these intermediate incremental representations are learned jointly, each layer being updated to follow both the representational needs of the layer above and the needs of the layer below.\n",
    "    - <font color=\"red\">Together, these two properties have made deep learning vastly more successful than previous approaches to machine learning.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.7 The modern machine-learning landscape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Why deep learning? Why now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://qph.ec.quoracdn.net/main-qimg-511694b55af00d23a19a8d11b27f0fe3.webp\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://blog.easysol.net/wp-content/uploads/2017/06/image1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://image.slidesharecdn.com/jeffdean-170115012221/95/jeff-dean-at-ai-frontiers-trends-and-developments-in-deep-learning-research-3-638.jpg?cb=1484674187\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://image.slidesharecdn.com/jeffdean-170115012221/95/jeff-dean-at-ai-frontiers-trends-and-developments-in-deep-learning-research-4-638.jpg?cb=1484674187\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://image.slidesharecdn.com/jeffdean-170115012221/95/jeff-dean-at-ai-frontiers-trends-and-developments-in-deep-learning-research-5-638.jpg?cb=1484674187\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Better <font color=\"red\">activation functions</font> for neural layers\n",
    "* Better <font color=\"red\">weight-initialization schemes</font>, starting with layer-wise pretraining, which was\n",
    "quickly abandoned\n",
    "* Better <font color=\"red\">optimization schemes</font>, such as RMSProp and Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4 A new wave of investment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.5 The democratization of deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.6 Will it last?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Simplicity\n",
    "* Scalability\n",
    "* Versatility and reusability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료\n",
    "* [1] Deep Learning with Python - https://www.manning.com/books/deep-learning-with-python\n",
    "* [2] Jupyter notebooks for the code samples of the book \"Deep Learning with Python\" - https://github.com/fchollet/deep-learning-with-python-notebooks\n",
    "* [3] Santford's class(2017) / CS231n: Convolutional Neural Networks for Visual Recognition / Lecture2 Image Classsification Pipeline - http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture2.pdf\n",
    "* [4] Santford's class(2017) / CS231n: Convolutional Neural Networks for Visual Recognition / Lecture 3: Loss Functions and Optimization - http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture3.pdf\n",
    "* [5] Santford's class(2017) / CS231n: Convolutional Neural Networks for Visual Recognition / Lecture 4: Backpropagation and Neural Networks - http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture4.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
